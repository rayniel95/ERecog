{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "8d86420431968fda625023edff1dc92564830918072732fc5e9bd707629c94be"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preparando el dataset\n",
    "Utilizaremos FairFace como dataset. Este posee las caracteristicas de que tiene igual proporcion en todas las etnias, a saber, Medio Oriente, Blancos, Negros, Hispanos-Latinos, Indios, Sur de Asia y Este de Asia. Al tener una buena proporcion de las etnias que generalmente no estan correctamente representadas no existira un bias significativo en el modelo. Las fotos han sido cropped por lo que este paso no es necesario. Debemos tener en cuenta que de procesar fotos nuevas estas tambien tiene que ser cropped utilizando dlib o alguna otra herramienta similar.\n",
    "\n",
    "No nos hara falta las fotos del Medio Oriente por lo que podemos desecharlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "train_csv = pandas.read_csv('./Datasets/FairFace/fairface_label_train.csv')\n",
    "test_csv = pandas.read_csv('./Datasets/FairFace/fairface_label_val.csv')\n",
    "\n",
    "train_csv = train_csv[train_csv['race'] != 'Middle Eastern']\n",
    "test_csv = test_csv[test_csv['race'] != 'Middle Eastern']"
   ]
  },
  {
   "source": [
    "Mezclamos las dos etnias asiaticas en una, ya que no se nos exige distinguirlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs_train = train_csv[(train_csv['race'] == 'East Asian') | (train_csv['race'] == 'Southeast Asian')].index\n",
    "\n",
    "train_csv.loc[indexs_train, 'race'] = 'Asian'\n",
    " \n",
    "indexs_test = test_csv[(test_csv['race'] == 'East Asian') | (test_csv['race'] == 'Southeast Asian')].index\n",
    "\n",
    "test_csv.loc[indexs_test, 'race'] = 'Asian'"
   ]
  },
  {
   "source": [
    "# Convirtiendo las imagenes a feature vectors\n",
    "Una vez tenemos el dataset preparado procedemos a convertir las imagenes a vectores de caracteristicas que sirvan de input para la red. Se carga la imagen en rgb y se resize a la size que se pide, se transforma esta en una matriz de 21x28 con vectores de longitud 3 por casilla, indicando cada valor en este vector el valor correspondiente al pixel en cada uno de los colores. Posteriormente se normalizan."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.image as image\n",
    "\n",
    "def getPixels(file):\n",
    "    img = image.load_img(file, target_size=(21, 28,))\n",
    "    return image.img_to_array(img)\n",
    "\n",
    "train_features = []\n",
    "for _, value in train_csv['file'].items():\n",
    "    train_features.append(getPixels('./Datasets/FairFace/' + value)/255)\n",
    "\n",
    "test_features = []\n",
    "for _, value in test_csv['file'].items():\n",
    "    test_features.append(getPixels('./Datasets/FairFace/' + value)/255)"
   ]
  },
  {
   "source": [
    "# Labels encoding\n",
    "Una vez listos los features vectors debemos codificar los labels en vectores one hot para el entrenamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_categories = {}\n",
    "\n",
    "for index, item in enumerate(train_csv['race'].unique()):\n",
    "    labels_categories[item] = index\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for _, item in train_csv['race'].items():\n",
    "    train_labels.append(labels_categories[item])\n",
    "\n",
    "for _, item in test_csv['race'].items():\n",
    "    test_labels.append(labels_categories[item])\n",
    "\n",
    "import keras.utils as utils\n",
    "\n",
    "one_hot_train = utils.to_categorical(train_labels, num_classes=5)\n",
    "one_hot_test = utils.to_categorical(test_labels, num_classes=5)"
   ]
  },
  {
   "source": [
    "# Creacion del train set y del validation set\n",
    "Teniendo el train set y el test set, ambos preparados con los feature vectors correspondientes, se procede a la creacion del set de validacion. Para ello nos guiamos por lo pedido en la orden y usamos el 80% para el train set, hacemos shuffle para randomizar el orden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "train_set, val_set, train_set_labels, val_set_labels = train_test_split(train_features, one_hot_train, train_size=0.80, shuffle=True)"
   ]
  },
  {
   "source": [
    "# Creacion de la red convolucional\n",
    "Una vez tengamos todo nuestro dataset procesado y listo podemos hacerle fit a la red. Pero primero hay que hacer una red :). Evidentemente los primero que pensamos es una convolucional puesto son las mejores en cuanto a clasificacion de imagenes. La idea obviamente va a ser probar con varias arquitecturas y parametros de las mismas para buscar la de mejor comportamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D, Flatten, Dense, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intentando crear vgg-face mas nunca esto termina\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(21, 28, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1939/1939 [==============================] - 855s 412ms/step - loss: 1.6505 - accuracy: 0.4080 - val_loss: 1.2271 - val_accuracy: 0.5172\n",
      "Epoch 2/10\n",
      "1939/1939 [==============================] - 772s 397ms/step - loss: 1.1747 - accuracy: 0.5401 - val_loss: 1.1543 - val_accuracy: 0.5529\n",
      "Epoch 3/10\n",
      "1939/1939 [==============================] - 806s 416ms/step - loss: 1.0772 - accuracy: 0.5877 - val_loss: 1.1170 - val_accuracy: 0.5704\n",
      "Epoch 4/10\n",
      "1939/1939 [==============================] - 808s 417ms/step - loss: 0.9952 - accuracy: 0.6241 - val_loss: 1.1119 - val_accuracy: 0.5744\n",
      "Epoch 5/10\n",
      "1939/1939 [==============================] - 805s 415ms/step - loss: 0.9051 - accuracy: 0.6673 - val_loss: 1.1448 - val_accuracy: 0.5771\n",
      "Epoch 6/10\n",
      "1939/1939 [==============================] - 718s 370ms/step - loss: 0.8076 - accuracy: 0.7114 - val_loss: 1.1142 - val_accuracy: 0.5915\n",
      "Epoch 7/10\n",
      "1939/1939 [==============================] - 782s 403ms/step - loss: 0.7096 - accuracy: 0.7546 - val_loss: 1.1815 - val_accuracy: 0.5961\n",
      "Epoch 8/10\n",
      "1939/1939 [==============================] - 788s 406ms/step - loss: 0.6118 - accuracy: 0.7959 - val_loss: 1.2667 - val_accuracy: 0.5844\n",
      "Epoch 9/10\n",
      "1939/1939 [==============================] - 797s 411ms/step - loss: 0.5146 - accuracy: 0.8385 - val_loss: 1.3839 - val_accuracy: 0.5827\n",
      "Epoch 10/10\n",
      "1939/1939 [==============================] - 804s 415ms/step - loss: 0.4225 - accuracy: 0.8756 - val_loss: 1.4862 - val_accuracy: 0.5859\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff0c472d280>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# convolucional simple\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(21, 28, 3), kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_set), np.array(train_set_labels), validation_data=(np.array(val_set), np.array(val_set_labels),), epochs=10, verbose=1)\n",
    "# no muy buena precision, necesitamos mas overfitting :)\n",
    "model.save('cnn_simple_model1.h5')"
   ]
  },
  {
   "source": [
    "# Carga de los modelos\n",
    "Se carga el modelo que se desee."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./cnn_simple_model1.h5')"
   ]
  },
  {
   "source": [
    "# Test de los modelos\n",
    "Una vez el modelo en memoria se procede a probar su presicion en el test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.579210863367728\n"
     ]
    }
   ],
   "source": [
    "predict_prob = model.predict(np.array(test_features))\n",
    "\n",
    "def tranToOneHotPredicted(predicted: np.array):\n",
    "    my_list = []\n",
    "    len_vectors = predicted[0].shape[0]\n",
    "    for index in range(predicted.shape[0]):\n",
    "        array = np.zeros((len_vectors,))\n",
    "        array[np.argmax(predicted[index])] = 1\n",
    "        my_list.append(array)\n",
    "    return np.array(my_list)\n",
    "\n",
    "predictions = tranToOneHotPredicted(predict_prob)\n",
    "\n",
    "print(precision_score(np.array(one_hot_test), predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}