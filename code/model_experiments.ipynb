{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "8d86420431968fda625023edff1dc92564830918072732fc5e9bd707629c94be"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preparando el dataset\n",
    "Utilizaremos FairFace como dataset. Este posee las caracteristicas de que tiene igual proporcion en todas las etnias, a saber, Medio Oriente, Blancos, Negros, Hispanos-Latinos, Indios, Sur de Asia y Este de Asia. Al tener una buena proporcion de las etnias que generalmente no estan correctamente representadas no existira un bias significativo en el modelo. Las fotos han sido cropped por lo que este paso no es necesario. Debemos tener en cuenta que de procesar fotos nuevas estas tambien tiene que ser cropped utilizando dlib o alguna otra herramienta similar.\n",
    "\n",
    "No nos hara falta las fotos del Medio Oriente por lo que podemos desecharlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "train_csv = pandas.read_csv('./Datasets/FairFace/fairface_label_train.csv')\n",
    "test_csv = pandas.read_csv('./Datasets/FairFace/fairface_label_val.csv')\n",
    "\n",
    "train_csv = train_csv[train_csv['race'] != 'Middle Eastern']\n",
    "test_csv = test_csv[test_csv['race'] != 'Middle Eastern']"
   ]
  },
  {
   "source": [
    "Mezclamos las dos etnias asiaticas en una, ya que no se nos exige distinguirlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs_train = train_csv[(train_csv['race'] == 'East Asian') | (train_csv['race'] == 'Southeast Asian')].index\n",
    "\n",
    "train_csv.loc[indexs_train, 'race'] = 'Asian'\n",
    " \n",
    "indexs_test = test_csv[(test_csv['race'] == 'East Asian') | (test_csv['race'] == 'Southeast Asian')].index\n",
    "\n",
    "test_csv.loc[indexs_test, 'race'] = 'Asian'\n"
   ]
  },
  {
   "source": [
    "# Convirtiendo las imagenes a feature vectors\n",
    "Una vez tenemos el dataset preparado procedemos a convertir las imagenes a vectores de caracteristicas que sirvan de input para la red. Se carga la imagen en rgb y se resize a la size que se pide, se transforma esta en una matriz de 21x28 con vectores de longitud 3 por casilla, indicando cada valor en este vector el valor correspondiente al pixel en cada uno de los colores. Posteriormente se normalizan."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.image as image\n",
    "\n",
    "def getImagePixels(file):\n",
    "    img = image.load_img(file, target_size=(21, 28,))\n",
    "    return image.img_to_array(img)\n",
    "\n",
    "train_features = []\n",
    "for _, value in train_csv['file'].items():\n",
    "    train_features.append(getImagePixels('./Datasets/FairFace/' + value)/255)\n",
    "\n",
    "test_features = []\n",
    "for _, value in test_csv['file'].items():\n",
    "    test_features.append(getImagePixels('./Datasets/FairFace/' + value)/255)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Labels encoding\n",
    "Una vez listos los features vectors debemos codificar los labels en vectores one hot para el entrenamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "test_labels = []\n",
    "\n"
   ]
  }
 ]
}