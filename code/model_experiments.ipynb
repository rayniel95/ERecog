{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "8d86420431968fda625023edff1dc92564830918072732fc5e9bd707629c94be"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Preparando el dataset\n",
    "Utilizaremos FairFace como dataset. Este posee las caracteristicas de que tiene igual proporcion en todas las etnias, a saber, Medio Oriente, Blancos, Negros, Hispanos-Latinos, Indios, Sur de Asia y Este de Asia. Al tener una buena proporcion de las etnias que generalmente no estan correctamente representadas no existira un bias significativo en el modelo. Las fotos han sido cropped por lo que este paso no es necesario. Debemos tener en cuenta que de procesar fotos nuevas estas tambien tiene que ser cropped utilizando dlib o alguna otra herramienta similar.\n",
    "\n",
    "No nos hara falta las fotos del Medio Oriente por lo que podemos desecharlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "train_csv = pandas.read_csv('./Datasets/FairFace/fairface_label_train.csv')\n",
    "test_csv = pandas.read_csv('./Datasets/FairFace/fairface_label_val.csv')\n",
    "\n",
    "train_csv = train_csv[train_csv['race'] != 'Middle Eastern']\n",
    "test_csv = test_csv[test_csv['race'] != 'Middle Eastern']"
   ]
  },
  {
   "source": [
    "Mezclamos las dos etnias asiaticas en una, ya que no se nos exige distinguirlas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs_train = train_csv[(train_csv['race'] == 'East Asian') | (train_csv['race'] == 'Southeast Asian')].index\n",
    "\n",
    "train_csv.loc[indexs_train, 'race'] = 'Asian'\n",
    " \n",
    "indexs_test = test_csv[(test_csv['race'] == 'East Asian') | (test_csv['race'] == 'Southeast Asian')].index\n",
    "\n",
    "test_csv.loc[indexs_test, 'race'] = 'Asian'"
   ]
  },
  {
   "source": [
    "# Convirtiendo las imagenes a feature vectors\n",
    "Una vez tenemos el dataset preparado procedemos a convertir las imagenes a vectores de caracteristicas que sirvan de input para la red. Se carga la imagen en rgb y se resize a la size que se pide, se transforma esta en una matriz de 21x28 con vectores de longitud 3 por casilla, indicando cada valor en este vector el valor correspondiente al pixel en cada uno de los colores. Posteriormente se normalizan."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.preprocessing.image as image\n",
    "\n",
    "def getPixels(file):\n",
    "    img = image.load_img(file, target_size=(21, 28,))\n",
    "    return image.img_to_array(img)\n",
    "\n",
    "train_features = []\n",
    "for _, value in train_csv['file'].items():\n",
    "    train_features.append(getPixels('./Datasets/FairFace/' + value)/255)\n",
    "\n",
    "test_features = []\n",
    "for _, value in test_csv['file'].items():\n",
    "    test_features.append(getPixels('./Datasets/FairFace/' + value)/255)"
   ]
  },
  {
   "source": [
    "# Labels encoding\n",
    "Una vez listos los features vectors debemos codificar los labels en vectores one hot para el entrenamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_categories = {}\n",
    "\n",
    "for index, item in enumerate(train_csv['race'].unique()):\n",
    "    labels_categories[item] = index\n",
    "\n",
    "train_labels = []\n",
    "test_labels = []\n",
    "\n",
    "for _, item in train_csv['race'].items():\n",
    "    train_labels.append(labels_categories[item])\n",
    "\n",
    "for _, item in test_csv['race'].items():\n",
    "    test_labels.append(labels_categories[item])\n",
    "\n",
    "import keras.utils as utils\n",
    "\n",
    "one_hot_train = utils.to_categorical(train_labels, num_classes=5)\n",
    "one_hot_test = utils.to_categorical(test_labels, num_classes=5)"
   ]
  },
  {
   "source": [
    "# Creacion del train set y del validation set\n",
    "Teniendo el train set y el test set, ambos preparados con los feature vectors correspondientes, se procede a la creacion del set de validacion. Para ello nos guiamos por lo pedido en la orden y usamos el 80% para el train set, hacemos shuffle para randomizar el orden."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "train_set, val_set, train_set_labels, val_set_labels = train_test_split(train_features, one_hot_train, train_size=0.80, shuffle=True)"
   ]
  },
  {
   "source": [
    "# Creacion de la red convolucional\n",
    "Una vez tengamos todo nuestro dataset procesado y listo podemos hacerle fit a la red. Pero primero hay que hacer una red :). Evidentemente los primero que pensamos es una convolucional puesto son las mejores en cuanto a clasificacion de imagenes. La idea obviamente va a ser probar con varias arquitecturas y parametros de las mismas para buscar la de mejor comportamiento."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D, Flatten, Dense, Dropout, MaxPooling2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intentando crear vgg-face mas nunca esto termina, cargar los pesos y hacer transfer learning\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(21, 28, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1939/1939 [==============================] - 855s 412ms/step - loss: 1.6505 - accuracy: 0.4080 - val_loss: 1.2271 - val_accuracy: 0.5172\n",
      "Epoch 2/10\n",
      "1939/1939 [==============================] - 772s 397ms/step - loss: 1.1747 - accuracy: 0.5401 - val_loss: 1.1543 - val_accuracy: 0.5529\n",
      "Epoch 3/10\n",
      "1939/1939 [==============================] - 806s 416ms/step - loss: 1.0772 - accuracy: 0.5877 - val_loss: 1.1170 - val_accuracy: 0.5704\n",
      "Epoch 4/10\n",
      "1939/1939 [==============================] - 808s 417ms/step - loss: 0.9952 - accuracy: 0.6241 - val_loss: 1.1119 - val_accuracy: 0.5744\n",
      "Epoch 5/10\n",
      "1939/1939 [==============================] - 805s 415ms/step - loss: 0.9051 - accuracy: 0.6673 - val_loss: 1.1448 - val_accuracy: 0.5771\n",
      "Epoch 6/10\n",
      "1939/1939 [==============================] - 718s 370ms/step - loss: 0.8076 - accuracy: 0.7114 - val_loss: 1.1142 - val_accuracy: 0.5915\n",
      "Epoch 7/10\n",
      "1939/1939 [==============================] - 782s 403ms/step - loss: 0.7096 - accuracy: 0.7546 - val_loss: 1.1815 - val_accuracy: 0.5961\n",
      "Epoch 8/10\n",
      "1939/1939 [==============================] - 788s 406ms/step - loss: 0.6118 - accuracy: 0.7959 - val_loss: 1.2667 - val_accuracy: 0.5844\n",
      "Epoch 9/10\n",
      "1939/1939 [==============================] - 797s 411ms/step - loss: 0.5146 - accuracy: 0.8385 - val_loss: 1.3839 - val_accuracy: 0.5827\n",
      "Epoch 10/10\n",
      "1939/1939 [==============================] - 804s 415ms/step - loss: 0.4225 - accuracy: 0.8756 - val_loss: 1.4862 - val_accuracy: 0.5859\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff0c472d280>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# convolucional simple1\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(21, 28, 3), kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_set), np.array(train_set_labels), validation_data=(np.array(val_set), np.array(val_set_labels),), epochs=10, verbose=1)\n",
    "# no muy buena precision, necesitamos mas overfitting :) y menos dropout y regularizacion\n",
    "model.save('cnn_simple_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1939/1939 [==============================] - 451s 232ms/step - loss: 1.4046 - accuracy: 0.4300 - val_loss: 1.1450 - val_accuracy: 0.5384\n",
      "Epoch 2/10\n",
      "1939/1939 [==============================] - 630s 325ms/step - loss: 1.1009 - accuracy: 0.5606 - val_loss: 1.0530 - val_accuracy: 0.5765\n",
      "Epoch 3/10\n",
      "1939/1939 [==============================] - 432s 223ms/step - loss: 0.9824 - accuracy: 0.6110 - val_loss: 1.0121 - val_accuracy: 0.5956\n",
      "Epoch 4/10\n",
      "1939/1939 [==============================] - 441s 227ms/step - loss: 0.8784 - accuracy: 0.6549 - val_loss: 1.0230 - val_accuracy: 0.5982\n",
      "Epoch 5/10\n",
      "1939/1939 [==============================] - 453s 234ms/step - loss: 0.7772 - accuracy: 0.6993 - val_loss: 1.0480 - val_accuracy: 0.6023\n",
      "Epoch 6/10\n",
      "1939/1939 [==============================] - 426s 220ms/step - loss: 0.6615 - accuracy: 0.7465 - val_loss: 1.1041 - val_accuracy: 0.5928\n",
      "Epoch 7/10\n",
      "1939/1939 [==============================] - 401s 207ms/step - loss: 0.5460 - accuracy: 0.7953 - val_loss: 1.2695 - val_accuracy: 0.5815\n",
      "Epoch 8/10\n",
      "1939/1939 [==============================] - 345s 178ms/step - loss: 0.4315 - accuracy: 0.8390 - val_loss: 1.3977 - val_accuracy: 0.5762\n",
      "Epoch 9/10\n",
      "1939/1939 [==============================] - 345s 178ms/step - loss: 0.3494 - accuracy: 0.8691 - val_loss: 1.5836 - val_accuracy: 0.5629\n",
      "Epoch 10/10\n",
      "1939/1939 [==============================] - 426s 220ms/step - loss: 0.2816 - accuracy: 0.8960 - val_loss: 1.7368 - val_accuracy: 0.5706\n"
     ]
    }
   ],
   "source": [
    "# convolucional simple2\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(21, 28, 3)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_set), np.array(train_set_labels), validation_data=(np.array(val_set), np.array(val_set_labels),), epochs=10, verbose=1)\n",
    "# este modelo lo que tiene es tremendo underfitting, no es problema de la regularizacion o del dropout parece que el dataset es demasiado grade y el modelo tiene muy pocas capas\n",
    "model.save('cnn_simple_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "2068/2068 [==============================] - 2125s 1s/step - loss: 1.5117 - accuracy: 0.3545 - val_loss: 1.2547 - val_accuracy: 0.4838\n",
      "Epoch 2/10\n",
      "2068/2068 [==============================] - 1935s 936ms/step - loss: 1.1904 - accuracy: 0.5179 - val_loss: 1.1459 - val_accuracy: 0.5382\n",
      "Epoch 3/10\n",
      "2068/2068 [==============================] - 1858s 898ms/step - loss: 1.0437 - accuracy: 0.5808 - val_loss: 1.0590 - val_accuracy: 0.5727\n",
      "Epoch 4/10\n",
      "2068/2068 [==============================] - 1797s 869ms/step - loss: 0.9408 - accuracy: 0.6258 - val_loss: 1.0372 - val_accuracy: 0.5920\n",
      "Epoch 5/10\n",
      "2068/2068 [==============================] - 1744s 843ms/step - loss: 0.8161 - accuracy: 0.6769 - val_loss: 1.0600 - val_accuracy: 0.5905\n",
      "Epoch 6/10\n",
      "2068/2068 [==============================] - 1683s 814ms/step - loss: 0.6835 - accuracy: 0.7338 - val_loss: 1.1945 - val_accuracy: 0.5744\n",
      "Epoch 7/10\n",
      "2068/2068 [==============================] - 1683s 814ms/step - loss: 0.5443 - accuracy: 0.7908 - val_loss: 1.2921 - val_accuracy: 0.5727\n",
      "Epoch 8/10\n",
      "2068/2068 [==============================] - 1692s 818ms/step - loss: 0.4124 - accuracy: 0.8434 - val_loss: 1.5392 - val_accuracy: 0.5770\n",
      "Epoch 9/10\n",
      "2068/2068 [==============================] - 1685s 815ms/step - loss: 0.3074 - accuracy: 0.8854 - val_loss: 1.9403 - val_accuracy: 0.5611\n",
      "Epoch 10/10\n",
      "2068/2068 [==============================] - 1776s 859ms/step - loss: 0.2381 - accuracy: 0.9133 - val_loss: 2.1370 - val_accuracy: 0.5539\n"
     ]
    }
   ],
   "source": [
    "# convolucional simple3\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(21, 28, 3)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(264, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_set), np.array(train_set_labels), validation_data=(np.array(val_set), np.array(val_set_labels),), batch_size=30, epochs=10, verbose=1)\n",
    "\n",
    "model.save('cnn_simple_model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "621/621 [==============================] - 347s 530ms/step - loss: 1.4530 - accuracy: 0.3682 - val_loss: 1.1678 - val_accuracy: 0.5282\n",
      "Epoch 2/10\n",
      "621/621 [==============================] - 340s 547ms/step - loss: 1.0958 - accuracy: 0.5623 - val_loss: 1.0283 - val_accuracy: 0.5904\n",
      "Epoch 3/10\n",
      "621/621 [==============================] - 317s 510ms/step - loss: 0.9657 - accuracy: 0.6163 - val_loss: 0.9872 - val_accuracy: 0.6093\n",
      "Epoch 4/10\n",
      "621/621 [==============================] - 338s 544ms/step - loss: 0.8873 - accuracy: 0.6512 - val_loss: 0.9631 - val_accuracy: 0.6243\n",
      "Epoch 5/10\n",
      "621/621 [==============================] - 346s 557ms/step - loss: 0.8310 - accuracy: 0.6737 - val_loss: 0.9350 - val_accuracy: 0.6371\n",
      "Epoch 6/10\n",
      "621/621 [==============================] - 346s 557ms/step - loss: 0.7590 - accuracy: 0.7037 - val_loss: 0.9389 - val_accuracy: 0.6383\n",
      "Epoch 7/10\n",
      "621/621 [==============================] - 339s 546ms/step - loss: 0.7044 - accuracy: 0.7254 - val_loss: 0.9640 - val_accuracy: 0.6359\n",
      "Epoch 8/10\n",
      "621/621 [==============================] - 334s 538ms/step - loss: 0.6293 - accuracy: 0.7569 - val_loss: 1.0190 - val_accuracy: 0.6327\n",
      "Epoch 9/10\n",
      "621/621 [==============================] - 336s 541ms/step - loss: 0.5473 - accuracy: 0.7885 - val_loss: 1.0388 - val_accuracy: 0.6310\n",
      "Epoch 10/10\n",
      "621/621 [==============================] - 344s 555ms/step - loss: 0.4837 - accuracy: 0.8169 - val_loss: 1.1371 - val_accuracy: 0.6239\n"
     ]
    }
   ],
   "source": [
    "# convolucional simple4\n",
    "model = Sequential()\n",
    "# probando con unas capas de maxpool2d\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(21, 28, 3,)))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2,)))\n",
    "model.add(Conv2D(264, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2,)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(train_set), np.array(train_set_labels), validation_data=(np.array(val_set), np.array(val_set_labels),), batch_size=100, epochs=10, verbose=1)\n",
    "\n",
    "model.save('cnn_simple_model4.h5')\n",
    "\n",
    "# el de mejor prediccion todo parece indicar que es necesario mas capas convolucionales y maxpooling ademas de aumentar el tamano del batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Carga de los modelos\n",
    "Se carga el modelo que se desee."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./cnn_simple_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./cnn_simple_model2.h5')"
   ]
  },
  {
   "source": [
    "model = load_model('./cnn_simple_model3.h5')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./cnn_simple_model4.h5')"
   ]
  },
  {
   "source": [
    "# Test de los modelos\n",
    "Una vez el modelo en memoria se procede a probar su presicion en el test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.6352816960096026\n"
     ]
    }
   ],
   "source": [
    "predict_prob = model.predict(np.array(test_features))\n",
    "\n",
    "def tranToOneHotPredicted(predicted: np.array):\n",
    "    my_list = []\n",
    "    len_vectors = predicted[0].shape[0]\n",
    "    for index in range(predicted.shape[0]):\n",
    "        array = np.zeros((len_vectors,))\n",
    "        array[np.argmax(predicted[index])] = 1\n",
    "        my_list.append(array)\n",
    "    return np.array(my_list)\n",
    "\n",
    "predictions = tranToOneHotPredicted(predict_prob)\n",
    "\n",
    "print(precision_score(np.array(one_hot_test), predictions, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}